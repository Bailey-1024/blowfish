
[{"content":" Scala # 至简原则 # 如果方法的返回值类型为 Unit，可以省略等号 = ；(如果期望是无返回值类型，可以省略=)\n​\t判断哪个正确：\ndef addNum(a:Int,b:Int):Unit {a+b} def addNum(a:Int,b:Int) {a+b} ​\t答：第一个方法是错误的，第二个是正确的。\n第一个方法是因为返回值类型和方法体中的表达式类型不匹配，加了:Unit有显式的返回类型，表达式需要和返回值类型保持一致\n第二个方法正确\n​\t判断打印出来的结果相同吗：\ndef addNum(a:Int,b:Int)={a+b}\rprintln(addNum(2,3)) def addNum(a:Int,b:Int) {a+b}\rprintln(addNum(2,3)) ​\t答：不同。第一个方法返回的值是5，第二个方法返回的是（）。\n第一个方法因为加了等于号，它会进行自动推断，推断出返回值类型是Int。\n第二个方法是因为没有等号，表达式没有被赋值或者返回，所以它默认返回值类型是Unit。\n如果匿名函数只有一个参数，小括号 () 和参数类型都可以省略，没有参数或参数超过一个的情况下不能省略 () ；\n简化下列函数：\n(x:Int)=\u0026gt;x+2 答：不能简化。\n如果参数只出现一次，且方法体或函数体没有嵌套使用参数，则参数可以用下划线 _ 来替代。 简化下面的方法:\nval list=(1,2,3,4)\rval result=list.map((x:Int)=\u0026gt;x+2) ​\t答：\nval list=(1,2,3,4)\rval result=list.map(_+2) ","date":"2024-08-02","externalUrl":null,"permalink":"/docs/speech-8-3/","section":"Docs","summary":"Scala # 至简原则 # 如果方法的返回值类型为 Unit，可以省略等号 = ；(如果期望是无返回值类型，可以省略=)","title":"演讲8-3","type":"docs"},{"content":" Scala # 基本常识 # 什么是Scala？ # ​\tScala 是一种运行在 JVM上的函数式的面向对象语言。\n优点 # Scala 是兼容的：兼容 Java，可以访问庞大的 Java 类库； Scala 是精简的：Scala 表达能力强，一行代码抵得上多行 Java 代码，开发速度快。可以让程序短小精悍，看起来更简洁，更优雅； Scala 是静态类型的：Scala 拥有非常先进的静态类型系统，支持类型推断和模式匹配等； Scala 可以开发大数据应用程序：例如 Spark、Flink 等。 基础要点 # 常量与变量 # ​\t常量关键词：val\n​\t变量关键词：var\n字符串 # 三引号 # ​\t作用：支持多行代码一起写，不需要拼接，直接在三引号内写即可。\n​\t举例：\nval|var 变量名 = \u0026#34;\u0026#34;\u0026#34;\r字符串1\r字符串2\r\u0026#34;\u0026#34;\u0026#34; 插值表达式 # ​\t作用：可以直接输出变量的值，不用拼接\n​\t举例：\nval|var 变量名 = s\u0026#34;${变量|表达式}字符串\u0026#34; 惰性赋值 # ​\t作用：定义完不会立马加载到JVM中。防止JVM内存消耗过大。（只有常量能使用）\n​\t举例：\nlazy val 变量名 = 表达式 类型转换 # ​\t强制转化：\nval|var 变量名 = 值.toXxx // 例如强转为 Int 则为 toInt for循环 # 语法格式：\nfor (i \u0026lt;- 表达式|数组|集合) {\r// 语句块(也叫循环体)\r} 举例：\n// 打印十次 HelloWorld\rfor (i \u0026lt;- 1 to 10) {\rprintln(s\u0026#34;HelloWorld! ${i}\u0026#34;)\r} 守卫 # ​\tfor 表达式中，可以添加 if 判断语句，这个 if 判断就称之为守卫。通过守卫可以让 for 表达式更加简洁。\n语法格式：\nfor (i \u0026lt;- 表达式|数组|集合 if 表达式) {\r// 语句块\r} 举例：\n// 打印 1~10 的偶数\rfor (i \u0026lt;- 1 to 10 if i % 2 == 0) {\rprintln(i)\r} yield 生成器 # ​\tyield 是一个类似 return 的关键字，但是 yield 不会结束函数，而 return 会结束函数。如果在循环结构中使用了 yield，相当于迭代一次遇到 yield 时就将 yield 后面(右边)的值放入一个集合，最后整个循环结束时将集合返回。我们把使用了 yield 的 for 表达式称之为推导式。yield不仅可以使用于 for 循环中，还可以使用于某个函数的参数，只要这个函数的参数允许被迭代。\n举例：\n// 将 1~10 的偶数返回\rval result = for (i \u0026lt;- 1 to 10 if i % 2 == 0) yield i\rprintln(result)\r// 生成 10，20，...，90，100\rval result = for (i \u0026lt;- 1 to 10) yield i * 10\rprintln(result) 实现 break # // 先导包\rimport scala.util.control.Breaks._\r// 当 i == 5 时结束循环\rbreakable {\rfor (i \u0026lt;- 1 to 10) {\rif (i == 5) break() else println(i)\r}\r} 实现continue # // 先导包\rimport scala.util.control.Breaks._\r// 当 i == 5 时跳过当次循环，继续下一次\rfor (i \u0026lt;- 1 to 10) {\rbreakable {\rif (i == 5) break() else println(i)\r}\r} 方法 # 语法格式：\ndef 方法名(参数名:参数类型, 参数名:参数类型, ...): [返回值类型] = {\r// 语句块(方法体)\r} 函数 # 语法格式：\n// 因为函数是对象，所以函数有类型：(函数参数类型1, 函数参数类型2,...) =\u0026gt; 函数返回值类型\rval 函数名: (函数参数类型1, 函数参数类型2,...) =\u0026gt; 函数返回值类型 = (参数名:参数类型, 参数名:参数类型, ...) =\u0026gt; {\r函数体\r} 方法和函数的区别：\n方法是隶属于类或者对象的，在运行时，它会被加载到 JVM 的方法区中 函数是一个对象，继承自 FunctionN，函数对象有 apply，curried，toString，tupled 这些方法，方法则没有。 Option # ​\t作用：避免空指针问题，有指定的值返回指定的值，没有就返回None。\n​\tScala 中，Option 类型表示可选值。这种类型的数据有两种形式：\nSome(x) ：表示实际的值\nNone ：表示没有值\n举例：\n​\t需求：\n定义一个两个数相除的方法，使用 Option 类型来封装结果。 当除数为零时，返回 None；当除数不为零时，返回 Some(相除)。 def division(a: Int, b: Int): Option[Int] = {\r// 定义一个两个数相除的方法，使用 Option 类型来封装结果\rif (b == 0) { // 当除数为零时，打印异常信息\rNone\r} else { // 当除数不为零时，打印相除结果\rSome(a / b)\r}\r}\rval a = 10\rval b = 0\rval result = division(10, 0)\r// 配合 Option 的 isEmpty 方法来检测元素是否为 None\rprintln(result.isEmpty)\r// 配合 Option 的 getOrElse 方法返回友好提示\rprintln(result.getOrElse(\u0026#34;对不起，除数不能为零\u0026#34;)) 面向对象 # 伴生对象 # ​\t一个 class 和 一个 object 具有相同的名字时，这个 object 就被称为伴生对象，这个 class 被称为伴生类。\n伴生对象和伴生类必须是相同的名字； 伴生对象和伴生类在同一个 scala 源文件中； 伴生对象和伴生类可以互相访问 private 私有属性。 private[this]访问权限 # ​\t只有当前类能访问，伴生对象无法访问。\napply方法 # 样例类 # ​\t语法：\ncase class 样例类名(val|var 成员属性1:类型1, 成员属性2:类型2, ...) {} 特点：\n特质 # 高级函数 # 至简原则 # 方法和函数不建议写 return 关键字，Scala 会使用函数体的最后一行代码作为返回值；\n方法的返回值类型如果能够推断出来，那么可以省略，如果有 return 则不能省略返回值类型，必须指定；\n因为函数是对象，所以函数有类型，但函数类型可以省略，Scala 编译期可以自动推断类型；\n如果方法明确声明了返回值为 Unit，那么即使方法体中有 return 关键字也不起作用；\n如果方法的返回值类型为 Unit，可以省略等号 = ；\n如果函数的参数类型能够推断出来，那么可以省略；\n如果方法体或函数体只有一行代码，可以省略花括号 {} ；\n如果方法无参，但是定义时声明了 () ，调用时小括号 () 可省可不省；\n如果方法无参，但是定义时没有声明 () ，调用时必须省略小括号 () ；\n如果不关心名称，只关心逻辑处理，那么函数名可以省略。也就是所谓的匿名函数；\n如果匿名函数只有一个参数，小括号 () 和参数类型都可以省略，没有参数或参数超过一个的情况下不能省略 () ；\n如果参数只出现一次，且方法体或函数体没有嵌套使用参数，则参数可以用下划线 _ 来替代。\n举例：\ndef main(args: Array[String]): Unit = {\r//方法和函数不建议写 return 关键字，Scala 会使用函数体的最后一行代码作为返回值；\rdef method1(a:Int,b:Int):Int={a+b}\rval func1:(Int,Int)=\u0026gt;Int=(a:Int,b:Int)=\u0026gt;{a-b}\r//方法的返回值类型如果能够推断出来，那么可以省略，如果有 return 则不能省略返回值类型，必须指定；\rdef method2(a:Int,b:Int)={a+b}\rdef method3(a:Int,b:Double):Double={\rreturn a+b\r}\r//因为函数是对象，所以函数有类型，但函数类型可以省略，Scala 编译期可以自动推断类型；\rval func2=(a:Int,b:Int)=\u0026gt;{a-b}\r//如果方法明确声明了返回值为 Unit，那么即使方法体中有 return 关键字也不起作用；\rdef method4(a:Int,b:Int):Unit={\rreturn a+b\r}\r//如果方法的返回值类型为 Unit，可以省略等号 = ；\rdef method5(a:Int,b:Int){a+b}\rprintln(method5(3,5))\r//如果函数的参数类型能够推断出来，那么可以省略；\rval func3=(a:Int,b:Int)=\u0026gt;{\ra-b\r}\rprintln(func3(6,2))\r//如果方法体或函数体只有一行代码，可以省略花括号 {} ；\rdef method6(a:Int,b:Int):Int=a+b\rval func4:(Int,Int)=\u0026gt;Int=(a,b)=\u0026gt;{a-b}\r//如果方法无参，但是定义时声明了 () ，调用时小括号 () 可省可不省；\rdef method7()={println(\u0026#34;hello\u0026#34;)}\rmethod7()\r//如果方法无参，但是定义时没有声明 () ，调用时必须省略小括号 () ；\rdef method8={println(\u0026#34;hello\u0026#34;)}\rmethod8\r//如果不关心名称，只关心逻辑处理，那么函数名可以省略。也就是所谓的匿名函数；\r// (a:Int)=\u0026gt;a*2\r//如果匿名函数只有一个参数，小括号 () 和参数类型都可以省略，没有参数或参数超过一个的情况下不能省略 () ；\rval list=(1 to 10).toList\rlist.map(a=\u0026gt;a+2)\r//如果参数只出现一次，且方法体或函数体没有嵌套使用参数，则参数可以用下划线 _ 来替代。\rprintln(list.map(_ + 2))\r} ","date":"2024-07-31","externalUrl":null,"permalink":"/docs/scala/","section":"Docs","summary":"Scala # 基本常识 # 什么是Scala？ # ​\tScala 是一种运行在 JVM上的函数式的面向对象语言。","title":"Scala笔记","type":"docs"},{"content":" 面试题 # Linux # 说 10 个常用的 Linux 命令\nLinux 系统中创建用户，用户组的命令\n​\t答：创建用户：useradd\n​\t创建用户组：groupadd\nLinux 修改文件所属的命令，修改文件权限的命令\n​\t答：修改文件所属的命令：chown 用户名：组名 文件名 ​ 修改用户权限的命令：chmod u/g/o+r/w/x 文件名 ​\tchmod u/g/o-r/w/x 文件名 ​ chmod 777 文件名\n用户目录在哪，环境变量有几种配置方式\n​\t答：用户目录：/home/用户名 ​\t配置环境变量的方式：3种 ​\t方式一：bash会话（当前会话有效）。 ​\t方式二：用户环境变量（当前用户生效）。 ​\t方式三：系统环境变量（所有用户生效）。\nLinux 安装软件的方式有几种，分别什么区别\n​\t答：安装软件的方式：4种 ​\t方式一：RPM ​\t方式二；YUM ​\t方式三：压缩包直接解压下载 ​\t方式四：源码编译下载 ​\t区别：RPM需要手动解决依赖问题，而YUM会自动解决依赖问题；源码下载需要自己选择安装地址，yum安装会自动安装在/etc目录下；压缩包下载，直接解压就行，但是需要自己配置环境变量\n如何选择 Linux 操作系统版本\n​\t答：现在主流的操作系统有Debian旗下收费版的Debian和免费版的Ubuntu，红帽旗下的收费的红帽和免费版的Centos。\n​\t如果是新手可以选择对用户友好的Ubuntu，用来学习的可以选择Centos；如果是企业可以选择收费版的Debian和红帽。\nLinux 服务器之间免密是如何实现的\n​\t答：1、生成密钥 ​ 2、检查密钥是否生成 ​ 3、生成成功后，将公钥，发送给需要免密的服务器 ​ 4、对方检查是否获取公钥，获取成功后，则对方可以进入自己的服务器\nShell 脚本第一行是什么，运行 Shell 脚本的方式有哪些，有什么区别\n​\t答：Shell脚本第一行：#!/bin/bash ​\t运行Shell脚本的方式： bash/sh shell脚本 ​ source/.\tshell脚本 ​ 使用路径 区别： ​\tbash/sh运行时会创建一个新的bash进程，变量无法共享；而source/.运行时不会创建新的进程，变量可以共享。 ​\t解决bash/sh运行脚本时变量无法共享，只需要在定义变量时，前面加上export。\nShell 脚本必须以 .sh 后缀结尾吗\n​\t答：不一定，文件以.sh后缀结尾是为了方便程序员知道这是一个脚本文件，只要文件内容的符合shell脚本的规则就可以。\nLinux 查看进程的命令以及杀死进程的命令\n​\t答：查看进程的命令：ps -ef\n​\t杀死进程的命令：kill -9 进程号 （杀死进程）\n​\tkill -15 进程号 （结束进程，当进程任务全部完成才结束）\nZookeeper # ZooKeeper 集群中有哪些角色，分别有什么作用\n​\t答：1、Leader领导者 ​\t作用：\n​\t1、提供读服务\n​\t2、可以参与选举\n​\t3、可以发起提议\n​\t4、可以广播数据给跟随者和观察者 2、Follower跟随者 ​\t作用：\n​\t1、提供读服务\n​\t2、可以参与选举\n​\t3、可以接受广播\n​\t4、可以转发提议给领导者，并且响应提议\t3、Observer观察者 ​\t作用：\n​\t1、提供读服务\n​\t2、可以接受广播\n​\t3、可以转发提议给领导者，不响应提议\n说说 ZooKeeper Znode 的特点\n​\t答：Znode是树形结构，数据通常很小，以KB为单位；数据以k-v形式存在，节点名为k，节点数据为v，znode的类型可以分为两大类，持久化节点和临时节点，创建节点时默认会创建持久化节点；持久化节点和临时节点中都有序列化节点，分别是序列化节点和临时序列化节点。序列化节点创建方式create -s 节点名，临时节点创建的方式时create -e，临时序列化节点创建方式时create -es。\n说说 ZooKeeper 的监听通知机制\n​\t答：ZooKeeper的监听通知机制是它的一项重要功能，用于在特定事件发生时通知客户端。ZooKeeper允许客户端设置监听器（listener）来监视znode节点上的变化。这些变化包括节点的创建、删除、更新等操作。当这些事件发生时，ZooKeeper会主动通知对应的客户端，以便它们能够及时响应和处理。\n设置监听器：客户端可以通过API设置一个监听器来监视特定的znode节点。例如，可以设置一个监听器来监听某个节点的创建、删除或数据更新事件。\n接收通知：当被监听的znode节点发生上述事件时，ZooKeeper会通过网络将这些事件实时推送到相关的客户端。\n处理事件：客户端收到通知后，可以根据具体的事件类型进行相应的处理。例如，如果某个节点的数据发生了更新，客户端可以重新读取该节点的最新数据。\n​\tZooKeeper的监听通知机制是其作为分布式协调服务的一个重要特性。它不仅增强了系统的实时性和响应能力，还简化了客户端与\tZooKeeper服务器之间的交互方式。通过设置监听器，客户端可以及时获取到znode节点的变化信息，从而做出相应的处理和决策。\n说一下 CAP 原则以及如何选择\n答:C是一致性\n​\tA是可用性（及时响应）\n​\tP是分区容错性\n​\tCAP原则三者只能满足其中的两个，所以有CA,CP,AP三种选择；需要保证数据的一致性的话，优先考虑C，需要保证及时响应的话优先考虑A，需要保证数据的容错性的话，优先考虑P。\nZooKeeper 的选主过程\n​\t答：ZooKeeper集群并不是直接基于Paxos算法实现的，而是采用了ZAB（ZooKeeper Atomic Broadcast）协议来确保数据的一致性和集群的可靠性。所以他拥有的角色是领导者、跟随者和观察者。 ​\t1、每个节点在启动时都会加入集群，并通过ZooKeeper的通信机制与其他节点建立连接。 ​\t2、zookeeper集群启动后，跟随者会发送请求成为领导者，谁先获得半票以上的支持就能成为领导者（观察者不参与投票） ​\t3、领导者发生宕机时，集群重新开始选主，会先过滤掉宕机跟随者，并且选出数据最全的跟随者（即Pid=公共Pid或者大于其他人的pid），接着在这些跟随者中进行无脑投票给节点id最大的，票数大于总服务器的二分之一，就能选出领导者 ​\t4、新的领导者将开始接受客户端的请求，并与其他跟随者节点同步数据，确保集群的一致性\nZooKeeper 如何帮助其他组件选主\n​\t答：ZooKeeper通过内置的ZAB协议和节点特性（如临时节点和临时序列化节点）来帮助其他组件进行选主。在选举过程中，每个参与选举的节点都会在ZooKeeper中创建相应的节点，并根据节点的特性（如序列号）来确定选举的优先级。最终，选举出的Leader节点将负责协调集群中的操作。\nHadoop # HDFS 的读取流程\n​\t答：1、客户端发起读取请求\n​\t2、NameNode检查文件是否存在，查询相关的元数据，通关网络拓补图找到较近的副本数据地址，返回给客户端\n​\t3、客户端与DataNode建立联系\n​\t4、DataNode发送数据给客户端，客户端读取完一个Block会再次发请求给NameNode，重复以上操作，直到读取完毕\nHDFS 的写入流程\n​\t答：1、客户端发起写入请求\n​\t2、NameNode处理请求，查看自己是否正常运行，再查看该文件是否存在，再检查客户端是否拥有写入的权限，检查没有问题，便会创建文件，响应客户端的请求\n​\t3、客户端将文件分块，开始写入数据\n​\t4、客服端通过NameNode知道那些DataNode中空闲，将 Block复制三份传入3个DataNode中\n​\t5、NameNode将文件与Block的映射关系存入磁盘中，将文件与Block的映射关系存入和Block与DataNode映射关系存入内存中\n​\t6、所有数据传入完毕，客户端关闭文件，NameNode更新元数据\n阐述 MapReduce 的计算流程\n​\t答：当处理数据时，数据会存入到HDFS上，HDFS会将数据进行分块，进入MapReduce框架中，数据会被分成split，每个split会被分配一个maptask，maptask中有一个环形缓存区（默认100M），缓存区以k-v的形式读取数据（k-偏移量，v-一行数据），当数据读入到缓存区的阈值（80%）时，就会将数据溢写进磁盘中，在磁盘中生成spill.out文件和spill.out.index文件,每次最多合并十个文件为file.out文件和file.out.index文件，当maptask完成总进度的百分之五，reducetask就会拉取数据到内存中（默认reducetask的百分之七十内存），当数据到达内存的阈值（66%），数据就会溢写进磁盘中，并进行合并，通过程序员重写reduce（）方法进行处理，最后将处理完的出去存入HDFS或者数据库中。\n阐述 YARN 集群的工作流程\n​\t答：\n为什么会产生 YARN 它解决了什么问题\n​\t答：\nHadoop MR 模型中数据倾斜一般是在 Mapper 端发生的还是在 Reducer 端发生的，为什么\n​\t答：发生在Reducer端。因为发生数据倾斜的可能是因为数据使用的是默认的Hashpartition，导致某些Reducer收到的数据明显多于其他Reducer，从而引起数据倾斜；数据本身就不平衡，导致其中一个Reduce数据的处理量较大。\nHadoop 常用的压缩算法有哪些，有什么区别\n​\t答：常用的压缩算法有：DEFLATE、BZip2、LZO、LZ4、Snappy、Zstd\nHadoop MR 模型中哪些地方可以进行优化\n​\t答：\n​\nHive # Hive SQL 的执行流程\n​\t答：1、用户首先输入SQL\n​\t2、HIve驱动器（Driver）中的解析器将sql解析会抽象语法树，并对抽象语法树进行语义分析\n​\t3、对抽象语法树进行优化\n​\t4、编译器（Compiler）将抽象语法树 （AST） 编译生成逻辑执行计划，接着再将逻辑计划优化并转换为物理计划\n​\t5、执行物理执行计划\n​\t6、返回结果\nHive 自定义函数的流程，有什么作用\n​\t答：自定义函数流程：\n​\t1、编写Java代码，创建一个java类，这个类需要根据自定义函数的类型（UDF、UDAF、UDTF）来继承相应的类，重写里面的initializ（用于定义输入和输出的参数类型），evaluate（用于实现具体的业务逻辑）等。\n​\t2、打包成jar包\n​\t3、上传到hive集群中\n​\t4、注册函数\n​\t5、使用函数\n​\t作用：\n​\t1、解决特定的业务需求\n​\t2、可以扩展HQL的功能\n​\t3、提高查询效率\n​\t4、提高代码的复用性和模块化\n说一说 Hive 的分区分桶\n​\t答：Hive中的分区和分桶是两种数据组织方式，它们各自服务于不同的目的，并有助于提高查询效率和数据管理。分区是通过将表内的数据按照具有相同或相似的属性进行分类，更多的是针对数据的存储路径；分桶是通过将表中的数据按照hash取余来进行分类，更多的是针对数据文件。\nHive 和传统数据库的区别\n​\t答： Hive 的物化视图和视图有什么区别\n​\t答：1、存储方式：视图是虚拟的表，不会存储数据；物化视图是真实的表，它存储了查询定义和查询结果集。\n​\t2、查询性能：视图只能简化sql语句，不会提高查询性能；物化视图可以通过预先计算，存储了计算结果，可以提高查询性能。\n​\t3、更新策略：视图是虚拟的，不支持更新操作；物化视图当原来的数据改变时会进行改变。\n​\nHive 内部表和外部表的区别\n​\t答：内部表：加载数据到 hive 所在的 hdfs 目录，删除时，元数据和数据文件都删除 ​\t外部表：不加载数据到 hive 所在的 hdfs 目录，删除时，只删除表结构。\nHive 导出数据有几种方式\n​\t答：insert overwrite、通过 HDFS 操作、export。\nHive 动态分区和静态分区有什么区别\n​\t答：静态分区与动态分区的主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断。详细来说，静态分区的列是在编译时期，通过用户传递来决定的；动态分区只有在 SQL执行时才能决定。\nHive 的 sort by、order by、distrbute by、cluster by 的区别\n答：1、Sort By：分区内有序；\n​\t2、Order By：全局排序，只有一个 Reducer；\n​\t3、Distrbute By：类似 MR 中 Partition，进行分区，结合 sort by 使用。\n​\t4、Cluster By：当 Distribute by 和 Sorts by 字段相同时，可以使用 Cluster by 方式。Cluster by 除了具有 Distribute by 的功能外还兼具 Sort by 的功能。但是排序只能是升序排序，不能指定排序规则为 ASC 或者 DESC。\n行式文件和列式文件的区别\n​\t答：1、存储方式：行式文件是一行所用字段存储在一起；列式文件是一列所以数值存储在一起。\n​\t2、查询效率：行式文件查询会将整行数据读取出来，会读一些不需要的数据，查询效率会受到影响；列式文件查询只会查询相应的列数据，查询效率高。\n​\t3、压缩效率：行式文件由于数据类型可能会有很多，压缩效率会降低；行式文件的数据类型相同，压缩效率高。\n​\t4、应用场景：行式文件适用于频繁进行插入、更新、删除等操作的场景；列示文件适用于大规模数据查询的场景。\n说一说常见的 SQL 优化\n​\t答：谓词下推、常量替换、列裁剪。\n根据现阶段所学知识总结一下产生数据倾斜的原因\n​\t答：1、读取压缩的不可分割的大文件\n​\t2、任务需要大量相同键的数据\n说一说 Hive 你知道的优化（问的几率不大，时间充足可以看看）\n​\nHBase # RowKey 如何设计，设计不好会产生什么后果\n​\t答：要遵行唯一原则、长度原则（10~100字节，越短越好）、散列原则\n​\t设计不好会产生的后果：\n​\t1、性能降低\n​\t2、数据倾斜\n​\t3、维护成本高\n列族如何设计，为什么不建议 HBase 设计过多列族\n​\t答：列族设计时列族的长度需要尽可能的短，列族数量尽量少（2~3个）。\nHBase 读取数据的流程\n​\t答：Client发出读取请求\u0026mdash;-\u0026gt;通过与zookeeper交互\u0026mdash;-\u0026gt;找到hbase:meta\u0026mdash;-\u0026gt;通过rowkey在hbase:meta上找到对应的hregion位置信息\u0026mdash;-\u0026gt;连接hregion所在的hregion server\u0026mdash;-\u0026gt;hregion server处理读取请求\u0026mdash;-\u0026gt;先在blockcache中查找数据，找到直接返回结果\u0026mdash;-\u0026gt;没找到便在memstore上查找数据，找到返回结果\u0026mdash;-\u0026gt;没找到就在hfile上查找\u0026mdash;-\u0026gt;找到后先将数据写入BlockCache\u0026mdash;-\u0026gt;返回结果\nHBase 写入数据的流程\n​\t答：Client发出读取请求\u0026mdash;-\u0026gt;通过与zookeeper交互\u0026mdash;-\u0026gt;找到hbase:meta\u0026mdash;-\u0026gt;通过rowkey在hbase:meta上找到对应的hregion位置信息\u0026mdash;-\u0026gt;连接hregion所在的hregion server\u0026mdash;-\u0026gt;hregion server处理写入请求\u0026mdash;-\u0026gt;数据会先写入hlog中\u0026mdash;-\u0026gt;然后再写入memstore中 \u0026mdash;-\u0026gt;返回成功结果\nHive 和 HBase 的区别\n为什么要使用 Phoenix\n答：1、构建在 HBase 上的 SQL 层\n​\t2、可以使用标准 SQL 在 HBase 中管理，便于开发\n​\t3、可以使用 JDBC 来创建表,插入数据、对 HBase 数据进行查询\n​\t4、Phoenix JDBC Driver 容易嵌入到支持 JDBC 的程序中\nHBase 的热点区域会产生什么问题\n​\t答：HBase 中热点问题其实就是数据倾斜问题，由于数据的分配不均匀，如 row key 设计的不合理导致数据过多集中于某一个或某几个 region server 上，会导致这些 region server 的访问压力，造成性能下降甚至不能够提供对外服务。\n​\t解决方式：建表时rowkey遵守设计原则，进行预分区。\n说一说 HBase 的数据刷写与合并\n​\t答：数据刷写过程：\n当MemStore中写入的数据达到阈值（默认128M）时 ==\u0026gt; 暂停向MemStore写入新的数据（通过内部状态控制） ==\u0026gt; 将MemStore中的数据写入HDFS中的HFile文件 ==\u0026gt; 更新Region元数据以反映新生成的HFile文件 ==\u0026gt; 恢复MemStore的写入功能\n​\t数据合并过程：\n1、当数据块达到 3 块，Hmaster 触发合并操作，Region 将数据块加载到本地，进行合并；\n2、 当合并的数据超过 256M，进行拆分，将拆分后的 Region 分配给不同的HregionServer 管理；\n3、 当 HregionServer 宕机后，将 HregionServer 上的 hlog 拆分，然后分配给不同的HregionServer 加载，修改hbase：meta；\n4、注意：HLOG会同步到HDFS\n","date":"2024-07-30","externalUrl":null,"permalink":"/docs/interview/","section":"Docs","summary":"面试题 # Linux # 说 10 个常用的 Linux 命令","title":"大数据面试题","type":"docs"},{"content":" 启动 # 启动 ZooKeeper（三台机器都需要执行，使用自己写的脚本） [root@node01 ~]# zookeeper start 或者zookeeper内置命令 zkServer.sh start zkServer.sh status 启动 HDFS + YARN。 [root@node01 ~]# start-all.sh 启动HBase start-hbase.sh 访问 # Web 访问：http://node01:16010 结果如下。\nWeb 访问：http://node02:16010 结果如下。\n关闭 # 关闭HBase [root@node01 ~]# stop-hbase.sh 关闭HDFS [root@node01 ~]# stop-all.sh 关闭Zookeeper [root@node01 ~]# zookeeper stop 或者zookeeper内置命令 zkServer.sh stop ","date":"2024-07-27","externalUrl":null,"permalink":"/docs/hbase-start/","section":"Docs","summary":"启动 # 启动 ZooKeeper（三台机器都需要执行，使用自己写的脚本） [root@node01 ~]# zookeeper start 或者zookeeper内置命令 zkServer.","title":"hbase启动步骤","type":"docs"},{"content":" 问题一：主从模式和主备模式的区别？ # 个人理解：\n主从模式和主备模式的区在于从节点和备用节点。\n从节点在主节点正常运行时，从节点会提供读服务。\n备用节点在主节点正常运行时，备用节点不会提供读服务。\nAI回答：\n问题二：学习Scala至简原则中发现的有趣小问题 # ​\t以下两个方法返回的结果相不相同？\n答：不相同，上面没有等于号的编译过程中不会进行推断，会默认返回类型是Unit，所以返回的是（）\n下面的有等于号的在编译过程中会进行推断，所以返回值是8\n","date":"2024-07-27","externalUrl":null,"permalink":"/docs/problems/","section":"Docs","summary":"问题一：主从模式和主备模式的区别？ # 个人理解：","title":"学习大数据时遇到的问题","type":"docs"},{"content":" HBase # 基本概念 # 什么是HBase # ​\tHBase是一个开源的、高可靠性、高性能、面向列（这里指列族，非列式存储）、可伸缩、实时读写的分布式数据库。\nHBase有什么用 # ​\tHBase拥有良好的分布式架构设计可以让海量数据进行快速存储，提供高效、可扩展的分布式存储解决方案，用于处理大规模的结构化或非结构化数据。\n特点 # 拓展 # HBse与RDBMS(关系数据库管理系统)的区别：\n数据模型 # ​\t在HBase表中一行数据一下几个属性：一个主键（RowKey），列族（Column Family）下的多个列（Column Qualifier），并且含有一个或多个以时间戳（TimeStamp）来实现的版本号（Version）。\n​\t数据模型图：\n​\t核心元素：\nNameSpace（命名空间） 命名空间类似于关系型数据库中的数据库的概念，他其实是表的逻辑分组。这种抽象为多租户相关功能奠定了基础。命名空间是可以管理维护的，可以创建，删除或更改命名空间。HBase 有两个特殊预定义的命名空间：\ndefault：没有明确指定命名空间的表将自动落入此命名空间\nhbase：系统命名空间，用于包含 HBase 的内部表和元数据表\nTable（表） Table 和关系型数据库中的表一个意思，由行和列组成。\nRowKey(主键)\nRowKey 的概念与关系型数据库中的主键相似，是一行数据的唯一标识。RowKey 可以是任意字符串（最大长度是=64KB，实际应用中长度一般为 10-100 Bytes），RowKey 以字节数组保存。存储数据时，数据会按照 RowKey 的字典序排序存储，所以设计 RowKey 时，要充分利用排序存储这个特性，将经常一起读取的行存放到一起。\n访问 HBase 数据的方式有三种：\n基于 RowKey 的单行查询； 基于 RowKey 的范围查询； 全表扫描查询。 Column Family（列族）\nColumn Family 即列族，HBase 基于列划分数据的物理存储，同一个列族中列的数据在物理上都存储在同一个 HFile中。一个列族可以包含任意多列，一般同一类的列会放在一个列族中，每个列族都有一组存储属性：|\n是否应该缓存在内存中； 数据如何被压缩或行键如何编码等。 Column Qualifier(列) 列族的限定词，理解为列的唯一标识。但是列标识是可以改变的，因此每一行可能有不同的列标识。使用的时候必须 (列族:列) ，列可以根据需求动态添加或者删除，同一个表中不同行的数据列都可以不同。\nTimestamp（时间戳） Timestamp是实现Hbase多个版本的关键。相同 RowKey 的数据按照 Timestamp 倒序排列，默认查询的是最新的版本，当然用户也可以指定 Timestamp 的值来读取指定版本的数据。 为了避免数据存在过多版本而造成管理（包括存贮和索引）负担，HBase 提供了两种数据版本回收方案： 一是保存数据的最后 n 个版本 二是保存最近一段时间内的版本（比如最近七天）\nCell（单元格） Cell 由 Row，Column Family，Column Qualifier，Version 组成。\n架构模型 # 总体架构 # ​\tHBase建立在Hadoop之上，利用Hadoop HDFS作为其底层存储系统，并借助Hadoop MapReduce提供高性能的数据处理能力。同时，HBase利用Zookeeper进行集群状态的监控、元数据的管理以及集群配置的维护。\n核心组件 # Zookeeper HBase 通过 ZooKeeper 来完成选举 HMaster、监控 HRegionServer、维护元数据集群配置等工作。主要工作职责如下：\n选举 HMaster：保证任何时候，集群中只有一个 HMaster。实现 HMaster 主从节点的 故障转移（Failover）； 监控 HRegionServer（节点探活）：实时监控 HRegionServer 的状态，将 HRegionServer 的上下线信息实时报告给Hmaster 维护元数据和集群配置：存放整个 HBase 集群的元数据以及集群的状态信息，包括： 存储所有 HRegion 的寻址入口（hbase:meta 元数据表），存储所有的的元数据信息； 存储 HBase 的 架构（Schema），包括有哪些 Table，每个 Table 有哪些 Column Family。 Client HBase Client 为用户提供了访问 HBase 的接口，可以通过元数据表（客户端负责发送请求到数据库）来定位到目标数据的 HRegionServer。客户端连接的方式有很多种： HBase shell和Java API。\n​\t发送的请求主要包括：\nDDL：数据库定义语言（表的建立，删除，添加删除列族，控制版本） DML：数据库操作语言（增删改） DQL：数据库查询语言（查询，全表扫描，基于主键，基于过滤器） HMaster HMaster 是 HBase 集群的主节点，负责整个集群的管理工作，HMaster 可以实现高可用（Active 和 Backup），通过 ZooKeeper 来维护主备节点的切换。\n**管理分配：**管理和分配 HRegion，负责启动的时候分配 HRegion 到具体的 HRegionServer，又或者在分割 HRegion 时关于新 HRegion 的分配。管理用户对 Table 结构的 DDL（创建，删除，修改）操作。 **负载均衡：**一方面负责将用户的数据均衡地分布在各个 HRegionServer 上，防止 HRegionServer 数据倾斜过载。另一方面负责将用户的请求均衡地分布在各个 HRegionServer 上，防止 HRegionServer 请求过热； **维护数据：**发现失效的 HRegion，并将失效的 HRegion 分配到正常的 HRegionServer 上。当某个 HRegionServer 下线时迁移其内部的 HRegion 到其他 HRegionServer 上。 权限控制。 HRegionServer\n​\tHRegionServer 直接对接用户的读写请求，是真正干活的节点，属于 HBase 具体数据的管理者。主要工作职责如下：\n实时和 HMaster 保持心跳，汇报当前节点的信息； 当接收到 HMaster 的命令创建表时，会分配一个 HRegion 对应一张表； 负责切分在运行过程中变得过大的 HRegion； 当 HRegionServer 意外关闭的时候，当前节点的 HRegion 会被其他 HRegionServer 管理； 维护 HMaster 分配给它的 HRegion，处理对这些 HRegion 的 IO 请求； 当客户端发送 DML 和 DQL 操作时，HRegionServer 负责和客户端建立连接； WAL：Write Ahead Log 日志先行。记录了数据写入、更新日志，它被用来做故障恢复； MemStore：写缓存，数据首先会被写入到 MemStore 中。每个 HRegion 的每个 Column Family 都会有一个MemStore 负责与底层的 HDFS 交互，存储数据（HLog、HFile）到 HDFS。 BlockCache：读缓存，在内存中存储了最常访问的数据，采用 LRU 机制进行淘汰。 HRegion\n​\t一个 HRegionServer 包含了多个 HRegion。HBase 将表中的数据基于 RowKey 的不同范围划分到不同 HRegion 上，每个 HRegion 都负责一定范围的数据存储和访问。\n​\tHRegion 是 HBase 中分布式存储和负载均衡的最小单元，不同的 HRegion 可以分布在不同的 HRegionServer 上。每个表一开始只有一个 HRegion，随着数据不断插入表，HRegion 不断增大，当增大到指定阀值（10G）的时候，HRegion 就会等分成两个HRegion，切分后其中一个 HRegion 会被转移到其他的 HRegionServer 上，实现负载均衡。\nStore\n​\t一个 HRegion 由多个 Store 组成，每个 Store 都对应一个 Column Family，Store 包含 1 个 MemStore 和 0 或多个StoreFile 组成。\nMemStore：作为 HBase 的内存数据存储，数据的写操作会先写到 MemStore 中，当 MemStore 中的数据增长到指定阈值（默认 128M）后，HRegionServer 会启动 FlushCache 进程将 MemStore 中的数据写入 StoreFile 持久化存储，每次写入后都形成一个单独的 StoreFile。当客户端检索数据时，先在 MemStore 中查找，如果 MemStore 中不存在，则会在StoreFile 中继续查找。 **StoreFile：**MemStore 中的数据写到文件后就是 StoreFile，StoreFile 底层是以 HFile 格式保存的。HBase 以 StoreFile 的大小来判断是否需要切分 HRegion。当一个 HRegion 中所有 StoreFile 的大小和数量都增长到超过指定阈值时，HMaster会把当前 HRegion 分割为两个，切分后其中一个 HRegion 会被转移到其他的 HRegionServer 上，实现负载均衡。 **HFile：**HFile 和 StoreFile 是同一个文件，只不过站在 HDFS 的角度称这个文件为 HFile，站在 HBase 的角度就称这个文 件为 StoreFile。是 HBase 在 HDFS 中存储数据的格式，它包含多层的索引，这样在 HBase 检索数据的时候就不用完全 的加载整个文件。 HFile\nHLog\n​\t一个 HRegionServer 只有一个 HLog 文件。负责记录数据的操作日志，当 HBase 出现故障时可以进行日志重放、故障恢复。例如磁盘掉电导致 MemStore 中的数据没有持久化存储到 StoreFile，这时就可以通过 HLog 日志重放来恢复数据。\n​\tHLog 文件就是一个普通的 Hadoop Sequence File，Sequece File 的 Key 是 HLogKey 对象，Sequece File 的 Value 是HBase 的 KeyValue 对象，即本次的操作。\n​\tHLogKey 中记录了写入数据的归属信息，除了 Table 和 HRegion 名称外，同时还包括 Sequence Number 和Timestamp：\nTimestamp：写入时间。\nSequence Number：起始值为 0，或者是最近一次存入文件系统中的 Sequence Number。\n数据被写入 WAL 后，会被加入到 MemStore 即写缓存。然后服务端就可以向客户端返回 ack 表示写数据完成。\nHDFS\n​\tHDFS 为 HBase 提供底层数据存储服务，同时为 HBase 提供高可用支持。HBase 将 HLog 存储在 HDFS 上，当服务器发生异常宕机时，可以重放 HLog 来恢复数据。\nBlockCache # 读写流程 # ​\tHBase 中单表的数据量通常可以达到 TB 级或 PB 级，但大多数情况下数据读取可以做到毫秒级。\n三层索引 # HBase0.96以前 # ​\tHBase 0.96 以前内部维护了两张特殊的表： -ROOT- 表和 .META. 表，用来查找各种表的 HRegion 位置。这两张特殊的表也像 HBase 中的其他表一样会切分成多个 HRegion。 -ROOT- 表比 .META. 更特殊一些，只能包含一个完整的HRegion信息，这样保证了只需要三次跳转，就能定位到任意 HRegion。\n-ROOT- ：记录 .META. 表的 HRegion 信息。 .META. ：记录用户的表的 HRegion 信息。 客户端查找HRegion信息流程：\n​\tClient==\u0026gt;Zookeeper==\u0026gt; -ROOT- ==\u0026gt;.Meta.==\u0026gt;HRegion\nHBase 0.96 以后 # ​\tHBase 0.96 以后，-ROOT- 表被移除，直接将 .META. 表 HRegion 位置信息存放在 ZooKeeper 中，并将 .META. 表更名为hbase:meta\n客户端查找HRegion信息流程：\n​\tClient==\u0026gt;Zookeepe ==\u0026gt;hbase:meta==\u0026gt;HRegion\nhbase表结构如下:\n读取数据流程 # ​\t客户端发出读取请求==\u0026gt;通过与zookeeper交互，找到rowkey所在的hregion位置信息==\u0026gt;连接管理hregion的hregion server==\u0026gt;hregion server处理读取请求==\u0026gt;先在block cache上查找，找到返回数据，没找到便在mem store上找 ==\u0026gt;mem stores上找到数据就返回数据，没找到就在hfile上找==\u0026gt;返回数据\n写入数据流程 # ​\t客户端发出写入请求==\u0026gt;通过与zookeeper交互,找到rowkey所在的hregion位置信息==\u0026gt;连接管理hregion的hregion server==\u0026gt;hregion server处理写入请求==\u0026gt;先将写入的数据写入hlog中==\u0026gt;再将数据写入memstore==\u0026gt;当memstore达到阈值进行刷写数据到hdfs中==\u0026gt;返回成功\n数据刷写 # 刷写时机 # 内存阈值 # ​\tHRegion中每个memstore超过128M（默认）就会进行数据刷写。在刷写的过程中，memstore还会阻塞所有写入该store的请求。\n内存总和 # ​\t当HRegionServer中所有的memstore阈值超过hbase堆内存的0.4*0.95大小，就会进行刷写，所有的写操作都会阻塞。\n日志阈值 # ​\tHBase 使用了 WAL 机制（日志先行），当数据到达 HRegion 时是先写入日志的，然后再被写入到 MemStore。如果日志的数量越来越大，这就意味着 MemStore 中未持久化到磁盘的数据越来越多。当 HRegionServer 挂掉的时候，恢复时间将会变得很长，所以有必要在日志到达一定的数量时进行一次刷写操作。相关公式为：Math.max(32, hbase_heapsize *hbase.regionserver.global.memstore.size * 2 / logRollSize)。\n定期刷写 # ​\t自己设置时间进行刷写，默认值 3600000 毫秒，即 1 小时。\n更新频率 # ​\tHRegion更新次数超过默认为 30000000 次，就会进行刷写。\n刷写策略 # ​\tHBASE 1.1 之前：MemStore 刷写是 HRegion 级别的。就是说，如果要刷写某个 MemStore ，MemStore 所在的 HRegion中其他的 MemStore 也是会被一起刷写的（简单的理解：Flush 一个列族时其它列族也会一起 Flush）。\n​\t重点：其他策略不满足要求时，也会退化到策略一。\n数据合并 # 合并类型 # Minor Compaction（次要/小） # ​\t选取一些小的、相邻的 StoreFile 将他们合并成一个更大的 StoreFile，在这个过程中不做任何删除数据、多版本数据的清理工作，但是会对 minVersion=0 并且设置 TTL 的过期版本数据进行清理。一次 Minor Compaction 的结果是让小的StoreFile 变的更少并且产生更大的 StoreFile。\nMajor Compaction（主要/大） # ​\t将所有的 StoreFile 合并成一个 StoreFile 清理三类无意义数据：被删除的数据、TTL 过期数据、版本号超过设定版本号的数据。一般情况下，Major Compaction 时间会持续比较长，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会关闭自动触发 Major Compaction 功能，改为手动在业务低峰期触发。\n总结：\nMinor Compaction：快速让小文件合并成大文件 Major Compaction：清理大文件不必要的数据，释放空间 合并时机 # ​\t触发 Compaction 的方式有三种：MemStore 刷盘、后台线程周期性检查、手动触发。\nMemStore 刷盘 # ​\tMemStore Flush 会产生 HFile 文件，文件越来越多就需要 Compact。每次执行完 Flush 操作之后，都会对当前 Store 中的文件数进行判断，一旦文件数大于配置，就会触发 Compaction。Compaction 都是以 Store 为单位进行的，整个 HRegion的所有 Store 都会执行 Compact。\n周期性检查 # ​\t后台线程定期触发检查是否需要执行 Compaction，检查周期可配置。线程先检查文件数是否大于配置，一旦大于就会触发 Compaction。如果不满足，它会接着检查是否满足 Major Compaction 条件（默认 7 天触发一次，可配置手动触发）。\n手动执行 # ​\t一般来讲，手动触发 Compaction 通常是为了执行 Major Compaction，一般有这些情况需要手动触发合并：\n因为很多业务担心自动 Major Compaction 影响读写性能（可以选择直接关闭），因此会选择低峰期手动触发； 用户在执行完 alter 操作之后希望立刻生效，手动执行触发 Major Compaction； HBase 管理员发现硬盘容量不够的情况下手动触发 Major Compaction 删除大量过期数据。 数据切分 # 切分原因 # 数据分布不均匀 # ​\t同一 HRegionServer 上数据文件越来越大，读请求也会越来越多。一旦所有的请求都落在同一个 HRegionServer 上，尤其是很多热点数据，必然会导致很严重的性能问题。\nCompaction 性能损耗严重 # ​\tCompaction 本质上是一个排序合并的操作，合并操作需要占用大量内存，因此文件越大，占用内存越多。Compaction 有可能需要迁移远程数据到本地进行处理（balance 之后的 Compaction 就会存在这样的场景），如果需要迁移的数据是大文件的话，带宽资源就会损耗严重。\n资源耗费严重 # ​\tHBase 的数据写入量也是很惊人的，每天都可能有上亿条的数据写入不做切分的话一个热点 HRegion 的新增数据量就有可能几十G，用不了多长时间大量读请求就会把单台 HRegionServer 的资源耗光。\n切分优化 # ​\t对于预估数据量较大的表，需要在创建表的时候根据 RowKey 执行 HRegion 的预分区。通过 HRegion 预分区，数据会 被均衡到多台机器上，这样可以一定程度解决热点应用数据量剧增导致的性能问题。\n","date":"2024-07-26","externalUrl":null,"permalink":"/docs/hbase/","section":"Docs","summary":"HBase # 基本概念 # 什么是HBase # ​\tHBase是一个开源的、高可靠性、高性能、面向列（这里指列族，非列式存储）、可伸缩、实时读写的分布式数据库。","title":"HBase笔记","type":"docs"},{"content":" Hive小问题 # Hive SQL 的执行流程 # 用户首先输入SQL HIve驱动器（Driver）中的解析器将sql解析会抽象语法树，并对抽象语法树进行语义分析 对抽象语法树进行优化 编译器（Compiler）将抽象语法树 （AST） 编译生成逻辑执行计划，接着再将逻辑计划优化并转换为物理计划 执行物理执行计划 返回结果 简化流程图：\nHive 自定义函数的流程，有什么作用 # 编写自定义函数的流程 # 编写Java代码，创建一个java类，这个类需要根据自定义函数的类型（UDF、UDAF、UDTF）来继承相应的类，重写里面的initializ（用于定义输入和输出的参数类型），evaluate（用于实现具体的业务逻辑）等。 打包成jar包 上传到hive集群中 注册函数 使用函数 自定义函数的作用 # 解决特定的业务需求 可以扩展HQL的功能 提高查询效率 提高代码的复用性和模块化 ","date":"2024-07-26","externalUrl":null,"permalink":"/docs/speech-7-26/","section":"Docs","summary":"Hive小问题 # Hive SQL 的执行流程 # 用户首先输入SQL HIve驱动器（Driver）中的解析器将sql解析会抽象语法树，并对抽象语法树进行语义分析 对抽象语法树进行优化 编译器（Compiler）将抽象语法树 （AST） 编译生成逻辑执行计划，接着再将逻辑计划优化并转换为物理计划 执行物理执行计划 返回结果 简化流程图：","title":"speech-7-26","type":"docs"},{"content":" 检查 MySQL 服务是否启动。 [root@node01 ~]# systemctl status mysqld\n启动 ZooKeeper（三台机器都需要执行，使用自己写的脚本） [root@node01 ~]# zookeeper start 或者zookeeper内置命令 zkServer.sh start zkServer.sh status\n启动 HDFS + YARN。 [root@node01 ~]# start-all.sh\n启动 JobHistory. [root@node01 ~]# mapred \u0026ndash;daemon start historyserver\n初始化 hive 数据库（第一次启动时执行）。 [root@node01 ~]# schematool -dbType mysql -initSchema\n启动 MetaStore 服务。\n​ 前台启动，学习期间推荐使用这种方式 [root@node01 ~]# hive \u0026ndash;service metastore\n​ 后台启动 [root@node01 ~]# nohup hive \u0026ndash;service metastore \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n启动 HiveServer2 服务。 前台启动，学习期间推荐使用这种方式 [root@node01 ~]# hiveserver2 后台启动 [root@node01 ~]# nohup hiveserver2 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n客户端连接 # 连接方式一 [root@node03 ~]# hive 退出命令行命令：exit; 客户端连接方式二 [root@node03 ~]# beeline -u jdbc:hive2://node01:10000 -n root 退出命令行命令：!exit 或者 !quit 关闭 # 先关闭 HiveServer2 服务和 MetaStore 服务（前台启动的话直接 Ctrl + C 即可）。\n再关闭 JobHistory 和 Hadoop。 [root@node01 ~]# mapred \u0026ndash;daemon stop historyserver [root@node01 ~]# stop-all.sh\n再关闭 ZooKeeper（三台机器都需要执行，使用自己写的脚本）。\n[root@node01 ~]# zookeeper stop 或者zookeeper内置命令 zkServer.sh stop\n","date":"2024-07-25","externalUrl":null,"permalink":"/docs/hive-start/","section":"Docs","summary":"检查 MySQL 服务是否启动。 [root@node01 ~]# systemctl status mysqld","title":"hive启动步骤","type":"docs"},{"content":"","date":"2024-08-02","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs","type":"docs"},{"content":"","date":"2024-08-02","externalUrl":null,"permalink":"/tags/scala/","section":"Tags","summary":"","title":"Scala","type":"tags"},{"content":"","date":"2024-08-02","externalUrl":null,"permalink":"/tags/speech/","section":"Tags","summary":"","title":"Speech","type":"tags"},{"content":"","date":"2024-08-02","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2024-08-02","externalUrl":null,"permalink":"/","section":"李","summary":"","title":"李","type":"page"},{"content":"","date":"2024-07-31","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024-07-31","externalUrl":null,"permalink":"/series/%E5%A4%A7%E6%95%B0%E6%8D%AE/","section":"Series","summary":"","title":"大数据","type":"series"},{"content":"","date":"2024-07-31","externalUrl":null,"permalink":"/tags/%E6%9D%8E/","section":"Tags","summary":"","title":"李","type":"tags"},{"content":"","date":"2024-07-30","externalUrl":null,"permalink":"/tags/interview/","section":"Tags","summary":"","title":"Interview","type":"tags"},{"content":"","date":"2024-07-30","externalUrl":null,"permalink":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/","section":"Tags","summary":"","title":"大数据","type":"tags"},{"content":"","date":"2024-07-27","externalUrl":null,"permalink":"/tags/hbase/","section":"Tags","summary":"","title":"Hbase","type":"tags"},{"content":"","date":"2024-07-27","externalUrl":null,"permalink":"/tags/problems/","section":"Tags","summary":"","title":"Problems","type":"tags"},{"content":"","date":"2024-07-26","externalUrl":null,"permalink":"/tags/hive/","section":"Tags","summary":"","title":"Hive","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]